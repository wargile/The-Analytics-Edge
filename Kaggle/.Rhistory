Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
varImp<-importance(AllRand)
sort(varImp[,1])
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days")
CorpusHeadline = Corpus(VectorSource(News$Headline))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, HStopWords)
#CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
CorpusHeadline = tm_map(CorpusHeadline, removeNumbers)
dtmHeadline = DocumentTermMatrix(CorpusHeadline,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseHeadline = removeSparseTerms(dtmHeadline, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparseHeadline))
rownames(HeadlineWords)<-NULL
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# add prefix "H" to Headline Words
colnames(HeadlineWords) = paste0("H", colnames(HeadlineWords))
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.98)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T, max_features=None, mtry=12)
?randomForest()
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T,mtry=12)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
varImp<-importance(AllRand)
sort(varImp[,1])
AllRand
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows")
CorpusHeadline = Corpus(VectorSource(News$Headline))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, HStopWords)
#CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
CorpusHeadline = tm_map(CorpusHeadline, removeNumbers)
dtmHeadline = DocumentTermMatrix(CorpusHeadline,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseHeadline = removeSparseTerms(dtmHeadline, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparseHeadline))
rownames(HeadlineWords)<-NULL
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# add prefix "H" to Headline Words
colnames(HeadlineWords) = paste0("H", colnames(HeadlineWords))
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.98)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
ncol(AllWordsTrain)
tune = tuneRF(x=AllWordsTrain[c(1:61)],  y= AllWordsTrain$Popular, mtryStart=6)
tune
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T,mtry=12)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
varImp<-importance(AllRand)
sort(varImp[,1])
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two")
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.98)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
names(AllWordsTrain)
ncol(AllWordsTrain)
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T,mtry=12)
tune = tuneRF(x=AllWordsTrain[c(1:59)],  y= AllWordsTrain$Popular, mtryStart=6)
tune
tune = tuneRF(x=AllWordsTrain[c(1:59)],  y= AllWordsTrain$Popular, mtryStart=4)
tune
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
AllRand
varImp<-importance(AllRand)
sort(varImp[,1])
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing", "take", "takes", "taking")
CorpusHeadline = Corpus(VectorSource(News$Headline))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, HStopWords)
#CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
CorpusHeadline = tm_map(CorpusHeadline, removeNumbers)
dtmHeadline = DocumentTermMatrix(CorpusHeadline,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseHeadline = removeSparseTerms(dtmHeadline, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparseHeadline))
rownames(HeadlineWords)<-NULL
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# add prefix "H" to Headline Words
colnames(HeadlineWords) = paste0("H", colnames(HeadlineWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
varImp<-importance(AllRand)
sort(varImp[,1])
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two", "get", "gets", "getting")
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.98)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T)
ncol(AllWordsTrain)
tune = tuneRF(x=AllWordsTrain[c(1:57)],  y= AllWordsTrain$Popular, mtryStart=4)
tune = tuneRF(x=AllWordsTrain[c(1:57)],  y= AllWordsTrain$Popular, mtryStart=6)
tune = tuneRF(x=AllWordsTrain[c(1:57)],  y= AllWordsTrain$Popular, mtryStart=8)
AllRand
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T,mtry=8)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
varImp<-importance(AllRand)
sort(varImp[,1])
ncol(AllWordsTest)
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = PredTestAllRand[,2])
write.csv(MySubmission, "SubmissionRandH99A98var57.csv", row.names=FALSE)
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importance=T)
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(PredTestAllRand[,2]>=0.5)
#new predict
Pred<-predict(AllRand,type="prob")
RocTrain <- prediction(Pred[,2],AllWordsTrain$Popular)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
tbl
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing", "take", "takes", "taking")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two", "get", "gets", "getting")
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
News <- NewsTrain
News$NewsDesk<- as.factor(News$NewsDesk)
News$SectionName<- as.factor(News$SectionName)
News$SubsectionName<- as.factor(News$SubsectionName)
News$PubDate = strptime(News$PubDate, "%Y-%m-%d %H:%M:%S")
News$Weekday = News$PubDate$wday
News$Weekday = as.factor(News$Weekday)
News$hour = News$PubDate$hour
News$hour = as.factor(News$hour)
News$Popular<- as.factor(News$Popular)
News$IsQ<- grepl("\\?", News$Headline)
News$IsQ <- as.factor(News$IsQ)
library(caTools)
split <- sample.split(News$Popular,SplitRatio=0.8)
NewsTrain <- subset(News,split == TRUE)
NewsTest <- subset(News,split == FALSE)
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing", "take", "takes", "taking")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two", "get", "gets", "getting")
CorpusHeadline = Corpus(VectorSource(News$Headline))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, HStopWords)
#CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
CorpusHeadline = tm_map(CorpusHeadline, removeNumbers)
dtmHeadline = DocumentTermMatrix(CorpusHeadline,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseHeadline = removeSparseTerms(dtmHeadline, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparseHeadline))
rownames(HeadlineWords)<-NULL
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
colnames(HeadlineWords) = paste0("H", colnames(HeadlineWords))
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.97)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = log(News$WordCount+1)
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
AllWords$Popular = News$Popular
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
set.seed(5123512)
AllRand = randomForest(Popular ~ ., data=AllWordsTrain,importantance=T)
#train
PredTrainAllRand = predict(AllRand, newdata=AllWordsTrain,type = "prob")
#table(PredTrainAllRand[,2]>=0.5)
tbl<-table(AllWordsTrain$Popular,PredTrainAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
#test
PredTestAllRand = predict(AllRand, newdata=AllWordsTest,type = "prob")
tbl<-table(AllWordsTest$Popular,PredTestAllRand[,2]>=0.5)
sum(diag(tbl))/sum(tbl)
# train AUC
RocTrain <- prediction(PredTrainAllRand[,2],AllWordsTrain$Popular)
#perf<- performance(RocTrainLog, "tpr", "fpr")
#plot(perf)
aucTrain <- as.numeric(performance(RocTrain,"auc")@y.values)
aucTrain
# test AUC
RocTest <- prediction(PredTestAllRand[,2],AllWordsTest$Popular)
#perf<- performance(RocTrain, "tpr", "fpr")
#plot(perf)
aucTest <- as.numeric(performance(RocTest,"auc")@y.values)
aucTest
AllRand
names(AllWords)
varImp<-importance(AllRand)
sort(varImp[,1])
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing", "take", "takes", "taking")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two", "get", "gets", "getting")
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
pop <- NewsTrain$Popular
idTrain <- NewsTrain$UniqueID
idTest <- NewsTest$UniqueID
# first remove Popular and ID from the Train and Test set
NewsTrain$Popular = NULL
NewsTrain$UniqueID = NULL
NewsTest$UniqueID = NULL
News <- rbind(NewsTrain,NewsTest)
#rbind() checks the rownames on the object it creates and adjusts duplicate rownames to make them unique.
rownames(News) <- NULL
nrow(News) == nrow(NewsTrain) + nrow(NewsTest)
# convert NewsDesk, SectioName, SubsectionName to factor
News$NewsDesk<- as.factor(News$NewsDesk)
News$SectionName<- as.factor(News$SectionName)
News$SubsectionName<- as.factor(News$SubsectionName)
News$PubDate = strptime(News$PubDate, "%Y-%m-%d %H:%M:%S")
# extract the Weekday from the Data and store in new variable
News$Weekday = News$PubDate$wday
# convert Weekday to factor
News$Weekday = as.factor(News$Weekday)
#extract the hour from the data and store in new variable
News$hour = News$PubDate$hour
News$hour = as.factor(News$hour)
News$IsQ<- grepl("\\?", News$Headline)
News$IsQ <- as.factor(News$IsQ)
News$WordCount <- log(News$WordCount+1)
NewsTrain<-head(News,nrow(NewsTrain))
NewsTest<-tail(News,nrow(NewsTest))
rownames(NewsTest)<-NULL
# add Popular (for Training) and UniqueID
NewsTrain$Popular <- c(pop)
NewsTrain$UniqueID<- c(idTrain)
NewsTest$UniqueID <- c(idTest)
NewsTrain$Popular<- as.factor(NewsTrain$Popular)
names(NewsTrain)
HStopWords <- c(stopwords("english"), "make","makes", "million",  "springsummer" ,   "paris",  "time", "times", "get","gets",  "getting", "bank" ,"year", "say", "says", "art", "raise","raised","raising", "raises", "big", "billion", "small", "show", "shows", "showing", "take", "takes", "taking")
AStopWords <- c(stopwords("english"), "take","takes","taking","work","works","working", "world",  "one",   "share", "sharing","shares", "bank", "banks", "look", "looks", "looking", "year", "years", "companies", "company", "plan", "plans", "planing", "now", "last", "citi","citys", "city",  "cities", "collect","collected","collects", "collection", "way", "like", "likes", "liked", "include","including","includes", "united", "unit", "group", "million", "business", "businesses", "help", "helps", "helping", "helped", "use", "uses", "useful", "using", "say", "says", "american", "americans", "houses", "housing", "said", "obama", "day", "days", "offers", "offer", "executes", "executive", "execute", "show", "shown", "shows", "make", "makes", "making", "two", "get", "gets", "getting")
CorpusHeadline = Corpus(VectorSource(News$Headline))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, HStopWords)
#CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
CorpusHeadline = tm_map(CorpusHeadline, removeNumbers)
dtmHeadline = DocumentTermMatrix(CorpusHeadline,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseHeadline = removeSparseTerms(dtmHeadline, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparseHeadline))
rownames(HeadlineWords)<-NULL
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# add prefix "H" to Headline Words
colnames(HeadlineWords) = paste0("H", colnames(HeadlineWords))
CorpusAbs = Corpus(VectorSource(News$Abstract))
CorpusAbs = tm_map(CorpusAbs, tolower)
CorpusAbs= tm_map(CorpusAbs, PlainTextDocument)
CorpusAbs = tm_map(CorpusAbs, removePunctuation)
CorpusAbs = tm_map(CorpusAbs, removeWords, AStopWords)
#CorpusAbs = tm_map(CorpusAbs, removeWords, stopwords("english"))
CorpusAbs = tm_map(CorpusAbs, stemDocument)
CorpusAbs = tm_map(CorpusAbs, removeNumbers)
dtmAbs = DocumentTermMatrix(CorpusAbs,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
sparseAbs = removeSparseTerms(dtmAbs, 0.98)
AbsWords = as.data.frame(as.matrix(sparseAbs))
colnames(AbsWords) = make.names(colnames(AbsWords))
colnames(AbsWords) = paste0("A", colnames(AbsWords))
AllWords <- cbind(HeadlineWords,AbsWords)
rownames(AllWords)<-NULL
AllWords$WordCount = News$WordCount
AllWords$Weekday = News$Weekday
AllWords$hour = News$hour
AllWords$NewsDesk <- News$NewsDesk
AllWords$SectionName <- News$SectionName
AllWords$SubsectionName <- News$SubsectionName
AllWords$IsQ <- News$IsQ
# Now we need to split the observations back into the training set and testing set.
AllWordsTrain = head(AllWords, nrow(NewsTrain))
AllWordsTest = tail(AllWords, nrow(NewsTest))
rownames(AllWordsTest)<-NULL
# Add dependent variable for Train
AllWordsTrain$Popular = NewsTrain$Popular
names(AllWords)
